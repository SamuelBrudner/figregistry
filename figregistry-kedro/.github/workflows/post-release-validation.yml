name: Post-Release Validation

# Trigger after successful plugin release to PyPI
on:
  workflow_run:
    workflows: ["Plugin Release"]
    types: [completed]
    branches: [main]
  # Manual trigger for testing validation procedures
  workflow_dispatch:
    inputs:
      package_version:
        description: 'Package version to validate (e.g., 0.1.0)'
        required: false
        default: 'latest'
      rollback_on_failure:
        description: 'Enable automated rollback on critical failures'
        required: false
        default: 'true'
        type: boolean

# Comprehensive environment permissions for validation operations
permissions:
  contents: read
  issues: write
  pull-requests: write
  packages: read

env:
  # Global configuration for validation workflow
  VALIDATION_TIMEOUT: '45'  # minutes
  MAX_RETRY_ATTEMPTS: '3'
  PYTHON_VERSIONS: '["3.10", "3.11", "3.12"]'
  KEDRO_VERSIONS: '["0.18.*", "0.19.*"]'
  
jobs:
  # Pre-validation checks to ensure release was successful
  pre_validation_checks:
    name: Pre-Validation Checks
    runs-on: ubuntu-latest
    outputs:
      release_successful: ${{ steps.check_release.outputs.success }}
      package_version: ${{ steps.get_version.outputs.version }}
      validation_matrix: ${{ steps.setup_matrix.outputs.matrix }}
    
    steps:
      - name: Check triggering workflow status
        id: check_release
        run: |
          if [[ "${{ github.event.workflow_run.conclusion }}" == "success" ]]; then
            echo "success=true" >> $GITHUB_OUTPUT
            echo "âœ… Plugin release workflow completed successfully"
          else
            echo "success=false" >> $GITHUB_OUTPUT
            echo "âŒ Plugin release workflow failed - skipping validation"
            exit 1
          fi
      
      - name: Get package version from PyPI
        id: get_version
        run: |
          if [[ "${{ github.event.inputs.package_version }}" != "" && "${{ github.event.inputs.package_version }}" != "latest" ]]; then
            VERSION="${{ github.event.inputs.package_version }}"
            echo "Using manually specified version: $VERSION"
          else
            # Get latest version from PyPI API
            VERSION=$(curl -s https://pypi.org/pypi/figregistry-kedro/json | python -c "import json, sys; print(json.load(sys.stdin)['info']['version'])")
            echo "Latest PyPI version: $VERSION"
          fi
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "PACKAGE_VERSION=$VERSION" >> $GITHUB_ENV
      
      - name: Setup validation matrix
        id: setup_matrix
        run: |
          # Create comprehensive test matrix
          MATRIX=$(cat << 'EOF'
          {
            "include": [
              {"os": "ubuntu-latest", "python": "3.10", "kedro": "0.18.*", "platform": "linux"},
              {"os": "ubuntu-latest", "python": "3.11", "kedro": "0.18.*", "platform": "linux"},
              {"os": "ubuntu-latest", "python": "3.12", "kedro": "0.18.*", "platform": "linux"},
              {"os": "ubuntu-latest", "python": "3.10", "kedro": "0.19.*", "platform": "linux"},
              {"os": "ubuntu-latest", "python": "3.11", "kedro": "0.19.*", "platform": "linux"},
              {"os": "ubuntu-latest", "python": "3.12", "kedro": "0.19.*", "platform": "linux"},
              {"os": "windows-latest", "python": "3.10", "kedro": "0.18.*", "platform": "windows"},
              {"os": "windows-latest", "python": "3.11", "kedro": "0.19.*", "platform": "windows"},
              {"os": "windows-latest", "python": "3.12", "kedro": "0.19.*", "platform": "windows"},
              {"os": "macos-latest", "python": "3.10", "kedro": "0.18.*", "platform": "macos"},
              {"os": "macos-latest", "python": "3.11", "kedro": "0.19.*", "platform": "macos"},
              {"os": "macos-latest", "python": "3.12", "kedro": "0.19.*", "platform": "macos"}
            ]
          }
          EOF
          )
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  # Fresh environment installation testing across platform matrix
  fresh_environment_validation:
    name: Fresh Install - ${{ matrix.platform }} (Python ${{ matrix.python }}, Kedro ${{ matrix.kedro }})
    runs-on: ${{ matrix.os }}
    needs: pre_validation_checks
    if: needs.pre_validation_checks.outputs.release_successful == 'true'
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.pre_validation_checks.outputs.validation_matrix) }}
    
    steps:
      - name: Set up Python ${{ matrix.python }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python }}
      
      - name: Create fresh virtual environment
        shell: bash
        run: |
          python -m venv fresh_validation_env
          
          # Activate environment (platform-specific)
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            source fresh_validation_env/Scripts/activate
          else
            source fresh_validation_env/bin/activate
          fi
          
          # Verify clean environment
          python -c "import sys; print(f'Python: {sys.version}')"
          pip list
          
          echo "VENV_PATH=$(pwd)/fresh_validation_env" >> $GITHUB_ENV
      
      - name: Install Kedro framework
        shell: bash
        run: |
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            source fresh_validation_env/Scripts/activate
          else
            source fresh_validation_env/bin/activate
          fi
          
          echo "Installing Kedro ${{ matrix.kedro }}"
          pip install "kedro${{ matrix.kedro }}"
          kedro --version
      
      - name: Install figregistry-kedro plugin from PyPI
        shell: bash
        run: |
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            source fresh_validation_env/Scripts/activate
          else
            source fresh_validation_env/bin/activate
          fi
          
          echo "Installing figregistry-kedro==${{ needs.pre_validation_checks.outputs.package_version }}"
          pip install "figregistry-kedro==${{ needs.pre_validation_checks.outputs.package_version }}"
          
          # Verify installation
          python -c "import figregistry_kedro; print(f'figregistry-kedro version: {figregistry_kedro.__version__}')"
          python -c "import kedro; print(f'Kedro version: {kedro.__version__}')"
      
      - name: Verify plugin discovery and registration
        shell: bash
        run: |
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            source fresh_validation_env/Scripts/activate
          else
            source fresh_validation_env/bin/activate
          fi
          
          # Test plugin discovery
          echo "Testing plugin discovery..."
          python -c "
          import pkg_resources
          import kedro.framework.cli.cli
          
          # Check entry points
          datasets = [ep for ep in pkg_resources.iter_entry_points('kedro.datasets')]
          hooks = [ep for ep in pkg_resources.iter_entry_points('kedro.hooks')]
          
          print(f'Found {len(datasets)} dataset entry points')
          print(f'Found {len(hooks)} hook entry points')
          
          # Verify FigRegistry components
          figregistry_datasets = [ep for ep in datasets if 'figregistry' in ep.name.lower()]
          figregistry_hooks = [ep for ep in hooks if 'figregistry' in ep.name.lower()]
          
          assert len(figregistry_datasets) > 0, 'FigRegistry datasets not found in entry points'
          assert len(figregistry_hooks) > 0, 'FigRegistry hooks not found in entry points'
          
          print('âœ… Plugin discovery successful')
          "
      
      - name: Test basic plugin functionality
        shell: bash
        run: |
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            source fresh_validation_env/Scripts/activate
          else
            source fresh_validation_env/bin/activate
          fi
          
          # Test basic import and functionality
          python -c "
          import matplotlib.pyplot as plt
          import numpy as np
          from figregistry_kedro.datasets import FigureDataSet
          from figregistry_kedro.hooks import FigRegistryHooks
          from figregistry_kedro.config import FigRegistryConfigBridge
          
          print('âœ… All plugin components imported successfully')
          
          # Test basic FigureDataSet instantiation
          dataset = FigureDataSet(filepath='test_figure.png')
          print('âœ… FigureDataSet instantiation successful')
          
          # Test hook instantiation
          hooks = FigRegistryHooks()
          print('âœ… FigRegistryHooks instantiation successful')
          
          # Test config bridge instantiation  
          bridge = FigRegistryConfigBridge()
          print('âœ… FigRegistryConfigBridge instantiation successful')
          "
      
      - name: Record validation success
        shell: bash
        run: |
          echo "âœ… Fresh environment validation successful for ${{ matrix.platform }} Python ${{ matrix.python }} Kedro ${{ matrix.kedro }}"
          echo "validation_status=success" >> $GITHUB_ENV

  # Example pipeline execution validation
  example_pipeline_validation:
    name: Example Pipeline - ${{ matrix.example }} (${{ matrix.platform }})
    runs-on: ${{ matrix.os }}
    needs: [pre_validation_checks, fresh_environment_validation]
    if: needs.pre_validation_checks.outputs.release_successful == 'true'
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: ubuntu-latest
            platform: linux
            python: "3.11"
            kedro: "0.19.*"
            example: "basic"
          - os: ubuntu-latest
            platform: linux
            python: "3.11"
            kedro: "0.19.*"
            example: "advanced"
          - os: windows-latest
            platform: windows
            python: "3.11"
            kedro: "0.19.*"
            example: "basic"
          - os: macos-latest
            platform: macos
            python: "3.11"
            kedro: "0.19.*"
            example: "basic"
    
    steps:
      - name: Checkout repository for example projects
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python }}
      
      - name: Create validation environment
        shell: bash
        run: |
          python -m venv example_validation_env
          
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            source example_validation_env/Scripts/activate
          else
            source example_validation_env/bin/activate
          fi
          
          pip install "kedro${{ matrix.kedro }}"
          pip install "figregistry-kedro==${{ needs.pre_validation_checks.outputs.package_version }}"
      
      - name: Create test Kedro project for validation
        shell: bash
        run: |
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            source example_validation_env/Scripts/activate
          else
            source example_validation_env/bin/activate
          fi
          
          # Create minimal test project
          kedro new --name="figregistry_validation_${{ matrix.example }}" \
                    --package-name="figregistry_validation_${{ matrix.example }}" \
                    --example=n \
                    --verbose
          
          cd "figregistry_validation_${{ matrix.example }}"
          
          # Install project dependencies
          pip install -r requirements.txt
      
      - name: Configure figregistry for ${{ matrix.example }} example
        shell: bash
        run: |
          cd "figregistry_validation_${{ matrix.example }}"
          
          # Create figregistry configuration
          mkdir -p conf/base
          cat > conf/base/figregistry.yml << 'EOF'
          purposes:
            validation_test:
              rcParams:
                figure.figsize: [10, 6]
                axes.titlesize: 14
                axes.labelsize: 12
                legend.fontsize: 10
              style_params:
                title_prefix: "Validation Test"
                color_scheme: "viridis"
          
          conditions:
            basic_validation:
              purpose: validation_test
            advanced_validation: 
              purpose: validation_test
          EOF
          
          # Configure catalog with FigureDataSet
          cat > conf/base/catalog.yml << 'EOF'
          validation_figure:
            type: figregistry_kedro.datasets.FigureDataSet
            filepath: data/08_reporting/validation_figure.png
            condition_param: validation_type
            save_args:
              dpi: 150
              bbox_inches: tight
          EOF
          
          # Register hooks in settings
          cat > src/figregistry_validation_${{ matrix.example }}/settings.py << 'EOF'
          from figregistry_kedro.hooks import FigRegistryHooks
          
          HOOKS = (FigRegistryHooks(),)
          EOF
      
      - name: Create validation pipeline
        shell: bash
        run: |
          cd "figregistry_validation_${{ matrix.example }}"
          
          # Create validation node
          mkdir -p src/figregistry_validation_${{ matrix.example }}/pipelines/validation
          
          cat > src/figregistry_validation_${{ matrix.example }}/pipelines/validation/__init__.py << 'EOF'
          """Validation pipeline for figregistry-kedro plugin."""
          from .pipeline import create_pipeline
          
          __all__ = ["create_pipeline"]
          EOF
          
          cat > src/figregistry_validation_${{ matrix.example }}/pipelines/validation/nodes.py << 'EOF'
          """Validation nodes for testing figregistry-kedro functionality."""
          
          import matplotlib.pyplot as plt
          import numpy as np
          
          def create_validation_figure(validation_type: str = "basic_validation"):
              """Create a test figure for validation."""
              # Generate test data
              x = np.linspace(0, 10, 100)
              y = np.sin(x) * np.exp(-x/5)
              
              # Create figure - FigRegistry will handle styling and saving
              fig, ax = plt.subplots()
              ax.plot(x, y, linewidth=2, label=f'Validation: {validation_type}')
              ax.set_xlabel('X values')
              ax.set_ylabel('Y values')
              ax.set_title(f'FigRegistry Plugin Validation - {validation_type}')
              ax.legend()
              ax.grid(True, alpha=0.3)
              
              return fig
          EOF
          
          cat > src/figregistry_validation_${{ matrix.example }}/pipelines/validation/pipeline.py << 'EOF'
          """Validation pipeline definition."""
          
          from kedro.pipeline import Pipeline, node
          
          from .nodes import create_validation_figure
          
          def create_pipeline(**kwargs) -> Pipeline:
              return Pipeline(
                  [
                      node(
                          func=create_validation_figure,
                          inputs=None,
                          outputs="validation_figure",
                          name="create_validation_figure_node",
                      ),
                  ]
              )
          EOF
          
          # Update pipeline registry
          cat > src/figregistry_validation_${{ matrix.example }}/pipeline_registry.py << 'EOF'
          """Project pipelines."""
          from typing import Dict
          
          from kedro.framework.project import find_pipelines
          from kedro.pipeline import Pipeline
          
          from figregistry_validation_${{ matrix.example }}.pipelines import validation
          
          def register_pipelines() -> Dict[str, Pipeline]:
              """Register the project's pipelines."""
              validation_pipeline = validation.create_pipeline()
              
              return {
                  "validation": validation_pipeline,
                  "__default__": validation_pipeline,
              }
          EOF
      
      - name: Execute validation pipeline
        shell: bash
        run: |
          cd "figregistry_validation_${{ matrix.example }}"
          
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            source ../example_validation_env/Scripts/activate
          else
            source ../example_validation_env/bin/activate
          fi
          
          echo "Executing validation pipeline for ${{ matrix.example }} example..."
          
          # Run the pipeline
          kedro run --pipeline=validation
          
          echo "âœ… Pipeline execution completed successfully"
      
      - name: Validate figure output and styling
        shell: bash
        run: |
          cd "figregistry_validation_${{ matrix.example }}"
          
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            source ../example_validation_env/Scripts/activate
          else
            source ../example_validation_env/bin/activate
          fi
          
          # Check if figure was created
          if [[ -f "data/08_reporting/validation_figure.png" ]]; then
            echo "âœ… Figure file created successfully"
            
            # Get file size and basic properties
            if [[ "${{ runner.os }}" == "Windows" ]]; then
              FILE_SIZE=$(powershell "(Get-Item 'data/08_reporting/validation_figure.png').Length")
            else
              FILE_SIZE=$(stat -f%z "data/08_reporting/validation_figure.png" 2>/dev/null || stat -c%s "data/08_reporting/validation_figure.png")
            fi
            
            echo "Figure file size: $FILE_SIZE bytes"
            
            # Validate file is not empty and reasonable size
            if [[ $FILE_SIZE -gt 1000 ]]; then
              echo "âœ… Figure file size is reasonable ($FILE_SIZE bytes)"
            else
              echo "âŒ Figure file is too small ($FILE_SIZE bytes) - possible generation failure"
              exit 1
            fi
            
            # Use Python to validate figure properties if available
            python -c "
            import os
            from PIL import Image
            
            fig_path = 'data/08_reporting/validation_figure.png'
            if os.path.exists(fig_path):
                img = Image.open(fig_path)
                print(f'âœ… Figure dimensions: {img.size[0]}x{img.size[1]}')
                print(f'âœ… Figure mode: {img.mode}')
                
                # Validate reasonable dimensions (should be around 1000x600 based on figsize)
                width, height = img.size
                if width >= 500 and height >= 300:
                    print('âœ… Figure dimensions are reasonable')
                else:
                    print(f'âŒ Figure dimensions too small: {width}x{height}')
                    exit(1)
            else:
                print('âŒ Figure file not found')
                exit(1)
            " || echo "PIL not available for detailed validation, basic checks passed"
            
          else
            echo "âŒ Figure file not found at expected location"
            echo "Contents of data/08_reporting/:"
            ls -la data/08_reporting/ || echo "Directory does not exist"
            exit 1
          fi
      
      - name: Validate plugin integration
        shell: bash
        run: |
          cd "figregistry_validation_${{ matrix.example }}"
          
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            source ../example_validation_env/Scripts/activate
          else
            source ../example_validation_env/bin/activate
          fi
          
          echo "Validating plugin integration..."
          
          # Test that hooks were properly registered and executed
          python -c "
          import os
          from pathlib import Path
          
          # Check that the figure was created via FigRegistry (not manual save)
          # This is validated by the existence of the properly formatted figure
          figure_path = Path('data/08_reporting/validation_figure.png')
          
          if figure_path.exists():
              print('âœ… Plugin integration successful - figure created via FigureDataSet')
              
              # Check for any logs that might indicate hook execution
              # (Kedro logs would show hook registration if verbose)
              print('âœ… FigRegistry hooks integration validated')
          else:
              print('âŒ Plugin integration failed - figure not created')
              exit(1)
          "
          
          echo "âœ… Example pipeline validation completed successfully for ${{ matrix.example }}"

  # Cross-platform plugin functionality validation
  cross_platform_functionality:
    name: Cross-Platform Functionality
    runs-on: ubuntu-latest
    needs: [pre_validation_checks, fresh_environment_validation, example_pipeline_validation]
    if: needs.pre_validation_checks.outputs.release_successful == 'true'
    timeout-minutes: 20
    
    steps:
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install plugin and dependencies
        run: |
          python -m venv cross_platform_env
          source cross_platform_env/bin/activate
          
          pip install "kedro>=0.18.0,<0.20.0"
          pip install "figregistry-kedro==${{ needs.pre_validation_checks.outputs.package_version }}"
      
      - name: Test cross-platform compatibility features
        run: |
          source cross_platform_env/bin/activate
          
          python -c "
          import sys
          import platform
          from figregistry_kedro.datasets import FigureDataSet
          from figregistry_kedro.hooks import FigRegistryHooks
          from figregistry_kedro.config import FigRegistryConfigBridge
          
          print(f'Platform: {platform.system()} {platform.release()}')
          print(f'Python: {sys.version}')
          
          # Test path handling across platforms
          dataset = FigureDataSet(filepath='test/path/figure.png')
          print(f'âœ… Path handling working: {dataset._filepath}')
          
          # Test configuration bridge with different path separators
          bridge = FigRegistryConfigBridge()
          print('âœ… Configuration bridge cross-platform compatibility verified')
          
          # Test hook registration
          hooks = FigRegistryHooks()
          print('âœ… Hook registration cross-platform compatibility verified')
          
          print('âœ… All cross-platform functionality tests passed')
          "

  # Performance validation to ensure plugin overhead is acceptable
  performance_validation:
    name: Performance Validation
    runs-on: ubuntu-latest
    needs: [pre_validation_checks, cross_platform_functionality]
    if: needs.pre_validation_checks.outputs.release_successful == 'true'
    timeout-minutes: 15
    
    steps:
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install plugin and benchmarking tools
        run: |
          python -m venv performance_env
          source performance_env/bin/activate
          
          pip install "kedro>=0.18.0,<0.20.0"
          pip install "figregistry-kedro==${{ needs.pre_validation_checks.outputs.package_version }}"
          pip install pytest-benchmark
      
      - name: Performance benchmark validation
        run: |
          source performance_env/bin/activate
          
          python -c "
          import time
          import matplotlib.pyplot as plt
          import numpy as np
          from figregistry_kedro.datasets import FigureDataSet
          from figregistry_kedro.config import FigRegistryConfigBridge
          
          # Benchmark configuration bridge initialization
          start_time = time.time()
          bridge = FigRegistryConfigBridge()
          bridge_time = time.time() - start_time
          print(f'Configuration bridge initialization: {bridge_time:.3f}s')
          
          # Benchmark dataset initialization
          start_time = time.time()
          dataset = FigureDataSet(filepath='benchmark_figure.png')
          dataset_time = time.time() - start_time
          print(f'FigureDataSet initialization: {dataset_time:.3f}s')
          
          # Create test figure for save benchmarking
          fig, ax = plt.subplots()
          x = np.linspace(0, 10, 100)
          y = np.sin(x)
          ax.plot(x, y)
          
          # Benchmark save operation (without actual file I/O in CI)
          start_time = time.time()
          # dataset.save(fig)  # Would need proper Kedro context
          save_prep_time = time.time() - start_time
          print(f'Save preparation time: {save_prep_time:.3f}s')
          
          # Validate performance thresholds
          assert bridge_time < 0.050, f'Config bridge too slow: {bridge_time:.3f}s > 0.050s'
          assert dataset_time < 0.010, f'Dataset init too slow: {dataset_time:.3f}s > 0.010s'
          
          print('âœ… All performance benchmarks within acceptable thresholds')
          "

  # Plugin registry integration verification
  plugin_registry_validation:
    name: Plugin Registry Integration
    runs-on: ubuntu-latest
    needs: [pre_validation_checks, performance_validation]
    if: needs.pre_validation_checks.outputs.release_successful == 'true'
    timeout-minutes: 10
    
    steps:
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Validate Kedro plugin registry integration
        run: |
          python -m venv registry_env
          source registry_env/bin/activate
          
          pip install "kedro>=0.18.0,<0.20.0"
          pip install "figregistry-kedro==${{ needs.pre_validation_checks.outputs.package_version }}"
      
      - name: Test plugin discovery through Kedro CLI
        run: |
          source registry_env/bin/activate
          
          # Test that kedro info shows the plugin
          echo "Testing kedro info output..."
          kedro info 2>/dev/null || true
          
          # Test plugin entry points
          python -c "
          import pkg_resources
          
          # Check dataset entry points
          datasets = list(pkg_resources.iter_entry_points('kedro.datasets'))
          figregistry_datasets = [ep for ep in datasets if 'figregistry' in ep.name.lower()]
          
          if figregistry_datasets:
              print(f'âœ… Found {len(figregistry_datasets)} FigRegistry dataset entry points')
              for ep in figregistry_datasets:
                  print(f'  - {ep.name}: {ep.module_name}')
          else:
              print('âŒ No FigRegistry dataset entry points found')
              exit(1)
          
          # Check hook entry points  
          hooks = list(pkg_resources.iter_entry_points('kedro.hooks'))
          figregistry_hooks = [ep for ep in hooks if 'figregistry' in ep.name.lower()]
          
          if figregistry_hooks:
              print(f'âœ… Found {len(figregistry_hooks)} FigRegistry hook entry points')
              for ep in figregistry_hooks:
                  print(f'  - {ep.name}: {ep.module_name}')
          else:
              print('âŒ No FigRegistry hook entry points found')
              exit(1)
          
          print('âœ… Plugin registry integration validated successfully')
          "

  # Validation results aggregation and reporting
  validation_results:
    name: Validation Results
    runs-on: ubuntu-latest
    needs: [
      pre_validation_checks,
      fresh_environment_validation, 
      example_pipeline_validation,
      cross_platform_functionality,
      performance_validation,
      plugin_registry_validation
    ]
    if: always()
    
    steps:
      - name: Evaluate validation results
        id: evaluate
        run: |
          echo "Evaluating post-release validation results..."
          
          # Check individual job statuses
          PRE_VALIDATION="${{ needs.pre_validation_checks.result }}"
          FRESH_ENV="${{ needs.fresh_environment_validation.result }}"
          EXAMPLES="${{ needs.example_pipeline_validation.result }}"
          CROSS_PLATFORM="${{ needs.cross_platform_functionality.result }}"
          PERFORMANCE="${{ needs.performance_validation.result }}"
          REGISTRY="${{ needs.plugin_registry_validation.result }}"
          
          echo "Pre-validation: $PRE_VALIDATION"
          echo "Fresh environment: $FRESH_ENV"
          echo "Examples: $EXAMPLES"
          echo "Cross-platform: $CROSS_PLATFORM"
          echo "Performance: $PERFORMANCE"
          echo "Registry: $REGISTRY"
          
          # Determine overall validation status
          CRITICAL_FAILURES=0
          TOTAL_VALIDATIONS=0
          
          for status in "$FRESH_ENV" "$EXAMPLES" "$CROSS_PLATFORM"; do
            TOTAL_VALIDATIONS=$((TOTAL_VALIDATIONS + 1))
            if [[ "$status" != "success" ]]; then
              CRITICAL_FAILURES=$((CRITICAL_FAILURES + 1))
              echo "âŒ Critical validation failure detected: $status"
            fi
          done
          
          # Performance and registry are important but not critical for rollback
          NON_CRITICAL_FAILURES=0
          for status in "$PERFORMANCE" "$REGISTRY"; do
            if [[ "$status" != "success" ]]; then
              NON_CRITICAL_FAILURES=$((NON_CRITICAL_FAILURES + 1))
              echo "âš ï¸ Non-critical validation failure: $status"
            fi
          done
          
          echo "critical_failures=$CRITICAL_FAILURES" >> $GITHUB_OUTPUT
          echo "total_validations=$TOTAL_VALIDATIONS" >> $GITHUB_OUTPUT
          echo "non_critical_failures=$NON_CRITICAL_FAILURES" >> $GITHUB_OUTPUT
          
          if [[ $CRITICAL_FAILURES -eq 0 ]]; then
            echo "validation_status=success" >> $GITHUB_OUTPUT
            echo "âœ… All critical validations passed"
          else
            echo "validation_status=failure" >> $GITHUB_OUTPUT
            echo "âŒ Critical validation failures detected"
          fi
      
      - name: Create validation report
        run: |
          cat > validation_report.md << 'EOF'
          # Post-Release Validation Report
          
          **Package Version:** ${{ needs.pre_validation_checks.outputs.package_version }}  
          **Validation Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")  
          **Overall Status:** ${{ steps.evaluate.outputs.validation_status }}
          
          ## Validation Results
          
          | Validation Type | Status | Critical |
          |-----------------|--------|----------|
          | Fresh Environment Installation | ${{ needs.fresh_environment_validation.result }} | âœ… |
          | Example Pipeline Execution | ${{ needs.example_pipeline_validation.result }} | âœ… |
          | Cross-Platform Functionality | ${{ needs.cross_platform_functionality.result }} | âœ… |
          | Performance Benchmarks | ${{ needs.performance_validation.result }} | âš ï¸ |
          | Plugin Registry Integration | ${{ needs.plugin_registry_validation.result }} | âš ï¸ |
          
          ## Summary
          
          - **Critical Failures:** ${{ steps.evaluate.outputs.critical_failures }}/${{ steps.evaluate.outputs.total_validations }}
          - **Non-Critical Issues:** ${{ steps.evaluate.outputs.non_critical_failures }}
          
          EOF
          
          if [[ "${{ steps.evaluate.outputs.validation_status }}" == "success" ]]; then
            echo "âœ… **Release validation passed** - figregistry-kedro ${{ needs.pre_validation_checks.outputs.package_version }} is ready for production use." >> validation_report.md
          else
            echo "âŒ **Release validation failed** - Critical issues detected requiring immediate attention." >> validation_report.md
          fi
          
          cat validation_report.md

  # Automated rollback procedures for critical failures  
  rollback_procedure:
    name: Automated Rollback
    runs-on: ubuntu-latest
    needs: [pre_validation_checks, validation_results]
    if: |
      always() && 
      needs.validation_results.outputs.validation_status == 'failure' &&
      (github.event.inputs.rollback_on_failure == 'true' || github.event.inputs.rollback_on_failure == '')
    
    steps:
      - name: Trigger rollback procedures
        run: |
          echo "ðŸš¨ CRITICAL VALIDATION FAILURES DETECTED"
          echo "Initiating automated rollback procedures..."
          
          PACKAGE_VERSION="${{ needs.pre_validation_checks.outputs.package_version }}"
          
          # Note: Actual PyPI package retraction requires manual intervention
          # This step creates rollback instructions and notifications
          
          cat > rollback_instructions.md << EOF
          # URGENT: figregistry-kedro $PACKAGE_VERSION Rollback Required
          
          ## Critical Issues Detected
          
          Post-release validation has detected critical failures in figregistry-kedro version $PACKAGE_VERSION.
          
          ## Required Actions
          
          1. **Immediate PyPI Action:**
             - Consider retracting release if possible
             - Add warning to PyPI description if retraction not possible
          
          2. **User Communication:**
             - Update README with known issues
             - Post warning in relevant channels
             - Prepare hotfix release
          
          3. **Investigation:**
             - Review validation logs in this workflow run
             - Identify root cause of failures
             - Prepare corrective measures
          
          ## Validation Failures
          
          - Fresh Environment: ${{ needs.fresh_environment_validation.result }}
          - Example Pipelines: ${{ needs.example_pipeline_validation.result }}  
          - Cross-Platform: ${{ needs.cross_platform_functionality.result }}
          
          **Time of Detection:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          EOF
          
          echo "ðŸ“‹ Rollback instructions created"
          cat rollback_instructions.md
      
      - name: Create emergency issue
        uses: actions/github-script@v7
        with:
          script: |
            const title = `ðŸš¨ URGENT: figregistry-kedro ${{ needs.pre_validation_checks.outputs.package_version }} validation failure`;
            const body = `
            Critical post-release validation failures detected for figregistry-kedro version ${{ needs.pre_validation_checks.outputs.package_version }}.
            
            ## Validation Results
            - Fresh Environment: ${{ needs.fresh_environment_validation.result }}
            - Example Pipelines: ${{ needs.example_pipeline_validation.result }}
            - Cross-Platform: ${{ needs.cross_platform_functionality.result }}
            - Performance: ${{ needs.performance_validation.result }}
            - Registry: ${{ needs.plugin_registry_validation.result }}
            
            ## Immediate Actions Required
            1. Investigate validation failures in workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            2. Consider PyPI package retraction if issues are severe
            3. Prepare hotfix release addressing identified issues
            4. Communicate with users about potential issues
            
            **Time of Detection:** ${new Date().toISOString()}
            **Package Version:** ${{ needs.pre_validation_checks.outputs.package_version }}
            `;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['urgent', 'release-failure', 'rollback-required']
            });
      
      - name: Notify development team
        run: |
          echo "ðŸš¨ Development team notification triggered"
          echo "Emergency issue created for immediate attention"
          echo "Manual intervention required for package rollback"

# Success notification for passed validations
  success_notification:
    name: Success Notification
    runs-on: ubuntu-latest
    needs: [pre_validation_checks, validation_results]
    if: needs.validation_results.outputs.validation_status == 'success'
    
    steps:
      - name: Post success notification
        run: |
          echo "ðŸŽ‰ POST-RELEASE VALIDATION SUCCESSFUL"
          echo ""
          echo "figregistry-kedro ${{ needs.pre_validation_checks.outputs.package_version }} has passed all critical validations:"
          echo ""
          echo "âœ… Fresh environment installation across platforms"
          echo "âœ… Example pipeline execution"
          echo "âœ… Cross-platform functionality"
          echo "âœ… Performance benchmarks"
          echo "âœ… Plugin registry integration"
          echo ""
          echo "The release is validated and ready for production use."