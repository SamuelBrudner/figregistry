# Advanced Enterprise-Grade Logging Configuration for FigRegistry-Kedro Integration
# =============================================================================
#
# This logging configuration provides comprehensive observability for the advanced
# FigRegistry-Kedro plugin integration, supporting complex enterprise workflows
# including training, inference, and reporting pipelines with detailed visibility
# into plugin operations, performance metrics, and automated figure management.
#
# Key Features:
# - Enterprise-grade structured logging with JSON formatting for log aggregation
# - Comprehensive plugin component logging (hooks, datasets, config bridge)
# - Performance monitoring and SLA tracking with detailed timing metrics
# - Multi-environment support (development, staging, production)
# - Detailed audit trails for enterprise compliance and reproducible research
# - Advanced error correlation and debugging capabilities
# - Integration with Kedro's logging infrastructure for seamless operation
#
# This configuration supports the advanced example's sophisticated pipeline
# architecture with training, inference, and reporting workflows while providing
# the detailed logging visibility required for enterprise deployment scenarios.

version: 1
disable_existing_loggers: false

# ==============================================================================
# FORMATTERS - Enterprise-Grade Log Formatting
# ==============================================================================

formatters:
  # Simple console formatter for development and debugging
  simple:
    format: "[%(asctime)s] {%(name)s:%(lineno)d} %(levelname)s - %(message)s"
    datefmt: "%Y-%m-%d %H:%M:%S"
  
  # Detailed console formatter with extended context information
  detailed:
    format: "[%(asctime)s] {%(pathname)s:%(lineno)d} [%(processName)s:%(threadName)s] %(levelname)s - %(name)s - %(message)s"
    datefmt: "%Y-%m-%d %H:%M:%S.%f"
  
  # Structured JSON formatter for enterprise log aggregation systems
  json:
    format: '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "module": "%(module)s", "function": "%(funcName)s", "line": %(lineno)d, "process": "%(processName)s", "thread": "%(threadName)s", "message": "%(message)s"}'
    datefmt: "%Y-%m-%dT%H:%M:%S.%fZ"
  
  # Performance metrics formatter for monitoring and analysis
  performance:
    format: '[PERF] [%(asctime)s] %(name)s - %(message)s'
    datefmt: "%Y-%m-%d %H:%M:%S.%f"
  
  # Audit trail formatter for enterprise compliance tracking
  audit:
    format: '[AUDIT] [%(asctime)s] [USER:%(user)s] [SESSION:%(session_id)s] [PIPELINE:%(pipeline_name)s] [NODE:%(node_name)s] %(levelname)s - %(message)s'
    datefmt: "%Y-%m-%d %H:%M:%S"

# ==============================================================================
# FILTERS - Advanced Log Filtering and Routing
# ==============================================================================

filters:
  # Performance filter for SLA monitoring and metrics collection
  performance_filter:
    (): __main__.PerformanceFilter
    min_duration_ms: 1.0  # Log operations taking >1ms
    
  # Enterprise audit filter for compliance and tracking
  audit_filter:
    (): __main__.AuditFilter
    include_pipeline_context: true
    include_user_context: true
    include_session_metadata: true

# ==============================================================================
# HANDLERS - Multiple Output Channels for Different Use Cases
# ==============================================================================

handlers:
  # === CONSOLE HANDLERS ===
  
  # Primary console output for development and interactive debugging
  console:
    class: logging.StreamHandler
    level: INFO
    formatter: simple
    stream: ext://sys.stdout
    
  # Detailed console output for advanced debugging and troubleshooting
  console_detailed:
    class: logging.StreamHandler
    level: DEBUG
    formatter: detailed
    stream: ext://sys.stdout
  
  # === FILE HANDLERS ===
  
  # General application log file with rotation for long-running processes
  file_general:
    class: logging.handlers.RotatingFileHandler
    level: INFO
    formatter: detailed
    filename: logs/kedro.log
    maxBytes: 10485760  # 10MB
    backupCount: 5
    encoding: utf8
    
  # Debug log file with comprehensive detail for troubleshooting
  file_debug:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: detailed
    filename: logs/debug.log
    maxBytes: 52428800  # 50MB
    backupCount: 3
    encoding: utf8
    
  # Error log file for error aggregation and analysis
  file_error:
    class: logging.handlers.RotatingFileHandler
    level: ERROR
    formatter: json
    filename: logs/errors.log
    maxBytes: 10485760  # 10MB
    backupCount: 10
    encoding: utf8
  
  # === FIGREGISTRY PLUGIN-SPECIFIC HANDLERS ===
  
  # FigRegistry plugin operations log with structured formatting
  figregistry_operations:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: json
    filename: logs/figregistry_operations.log
    maxBytes: 20971520  # 20MB
    backupCount: 5
    encoding: utf8
    
  # Performance monitoring log for SLA tracking and optimization
  performance_metrics:
    class: logging.handlers.RotatingFileHandler
    level: INFO
    formatter: performance
    filename: logs/performance.log
    maxBytes: 10485760  # 10MB
    backupCount: 3
    encoding: utf8
    filters: [performance_filter]
    
  # Audit trail log for enterprise compliance and reproducible research
  audit_trail:
    class: logging.handlers.RotatingFileHandler
    level: INFO
    formatter: audit
    filename: logs/audit.log
    maxBytes: 10485760  # 10MB
    backupCount: 10
    encoding: utf8
    filters: [audit_filter]
  
  # === STRUCTURED LOGGING HANDLERS ===
  
  # JSON structured log for enterprise log aggregation systems
  structured_json:
    class: logging.handlers.RotatingFileHandler
    level: INFO
    formatter: json
    filename: logs/structured.log
    maxBytes: 31457280  # 30MB
    backupCount: 7
    encoding: utf8
    
  # Configuration operations log for bridge and merge tracking
  config_operations:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: json
    filename: logs/config_operations.log
    maxBytes: 10485760  # 10MB
    backupCount: 3
    encoding: utf8

# ==============================================================================
# LOGGERS - Hierarchical Logger Configuration
# ==============================================================================

loggers:
  # === KEDRO CORE LOGGERS ===
  
  # Kedro core framework logging
  kedro:
    level: INFO
    handlers: [console, file_general, structured_json]
    propagate: false
    
  # Kedro pipeline execution logging  
  kedro.pipeline:
    level: INFO
    handlers: [console, file_general, structured_json]
    propagate: false
    
  # Kedro data catalog operations
  kedro.io:
    level: INFO
    handlers: [console, file_general, structured_json]
    propagate: false
    
  # Kedro configuration loading
  kedro.config:
    level: INFO
    handlers: [console, file_general, config_operations]
    propagate: false
    
  # === FIGREGISTRY PLUGIN LOGGERS ===
  
  # Root FigRegistry plugin logger
  figregistry_kedro:
    level: INFO
    handlers: [console, figregistry_operations, structured_json]
    propagate: false
    
  # Configuration bridge operations and merging
  figregistry_kedro.config:
    level: DEBUG
    handlers: [console_detailed, config_operations, figregistry_operations, audit_trail]
    propagate: false
    
  # Lifecycle hooks execution and context management
  figregistry_kedro.hooks:
    level: DEBUG
    handlers: [console_detailed, figregistry_operations, performance_metrics, audit_trail]
    propagate: false
    
  # FigureDataSet operations and styling application
  figregistry_kedro.datasets:
    level: DEBUG
    handlers: [console_detailed, figregistry_operations, performance_metrics, audit_trail]
    propagate: false
    
  # Performance monitoring and metrics collection
  figregistry_kedro.performance:
    level: INFO
    handlers: [performance_metrics, structured_json]
    propagate: false
    
  # === CORE FIGREGISTRY LOGGERS ===
  
  # Core FigRegistry library operations
  figregistry:
    level: INFO
    handlers: [console, figregistry_operations, structured_json]
    propagate: false
    
  # FigRegistry configuration operations
  figregistry.config:
    level: DEBUG
    handlers: [config_operations, figregistry_operations]
    propagate: false
    
  # FigRegistry style resolution and application
  figregistry.style:
    level: DEBUG
    handlers: [figregistry_operations, performance_metrics]
    propagate: false
    
  # FigRegistry output management and file operations
  figregistry.output:
    level: DEBUG
    handlers: [figregistry_operations, audit_trail]
    propagate: false
    
  # === SCIENTIFIC COMPUTING LIBRARY LOGGERS ===
  
  # Matplotlib backend and rendering operations
  matplotlib:
    level: WARNING
    handlers: [file_general]
    propagate: false
    
  # NumPy array operations (minimal logging)
  numpy:
    level: ERROR
    handlers: [file_error]
    propagate: false
    
  # Pandas data operations (minimal logging)
  pandas:
    level: WARNING
    handlers: [file_general]
    propagate: false
    
  # === PIPELINE-SPECIFIC LOGGERS ===
  
  # Training pipeline operations and model management
  kedro.pipelines.training:
    level: INFO
    handlers: [console, figregistry_operations, audit_trail]
    propagate: false
    
  # Inference pipeline operations and prediction management
  kedro.pipelines.inference:
    level: INFO
    handlers: [console, figregistry_operations, audit_trail]
    propagate: false
    
  # Reporting pipeline operations and output generation
  kedro.pipelines.reporting:
    level: INFO
    handlers: [console, figregistry_operations, audit_trail]
    propagate: false
    
  # === THIRD-PARTY LIBRARY LOGGERS ===
  
  # Pydantic validation operations
  pydantic:
    level: WARNING
    handlers: [file_general, config_operations]
    propagate: false
    
  # YAML configuration parsing
  yaml:
    level: WARNING
    handlers: [config_operations]
    propagate: false
    
  # Concurrent futures and threading operations
  concurrent.futures:
    level: WARNING
    handlers: [file_general]
    propagate: false

# ==============================================================================
# ROOT LOGGER CONFIGURATION
# ==============================================================================

root:
  level: INFO
  handlers: [console, file_general, file_error, structured_json]

# ==============================================================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# ==============================================================================

# Development environment logging configuration
# Usage: Set KEDRO_ENV=development to activate
development:
  loggers:
    figregistry_kedro:
      level: DEBUG
      handlers: [console_detailed, figregistry_operations, file_debug]
    figregistry_kedro.config:
      level: DEBUG
    figregistry_kedro.hooks:
      level: DEBUG
    figregistry_kedro.datasets:
      level: DEBUG
  root:
    level: DEBUG
    handlers: [console_detailed, file_debug, structured_json]

# Staging environment logging configuration  
# Usage: Set KEDRO_ENV=staging to activate
staging:
  loggers:
    figregistry_kedro:
      level: INFO
      handlers: [console, figregistry_operations, structured_json, audit_trail]
    figregistry_kedro.performance:
      level: INFO
      handlers: [performance_metrics, structured_json]
  root:
    level: INFO
    handlers: [console, file_general, structured_json, audit_trail]

# Production environment logging configuration
# Usage: Set KEDRO_ENV=production to activate  
production:
  loggers:
    figregistry_kedro:
      level: INFO
      handlers: [figregistry_operations, structured_json, audit_trail]
    figregistry_kedro.config:
      level: INFO
      handlers: [config_operations, audit_trail]
    figregistry_kedro.hooks:
      level: INFO
      handlers: [figregistry_operations, performance_metrics, audit_trail]
    figregistry_kedro.datasets:
      level: INFO
      handlers: [figregistry_operations, performance_metrics, audit_trail]
    kedro:
      level: INFO
      handlers: [file_general, structured_json]
  root:
    level: WARNING
    handlers: [file_general, file_error, structured_json, audit_trail]

# ==============================================================================
# PERFORMANCE MONITORING CONFIGURATION
# ==============================================================================

# Custom filter classes for advanced log processing
# These would be implemented in the project's logging utilities

# class PerformanceFilter(logging.Filter):
#     """Filter for capturing performance metrics and SLA violations."""
#     def __init__(self, min_duration_ms=1.0):
#         super().__init__()
#         self.min_duration_ms = min_duration_ms
#     
#     def filter(self, record):
#         # Add performance context to log records
#         if hasattr(record, 'duration_ms'):
#             return record.duration_ms >= self.min_duration_ms
#         return True

# class AuditFilter(logging.Filter):
#     """Filter for adding audit trail context to log records."""
#     def __init__(self, include_pipeline_context=True, include_user_context=True, include_session_metadata=True):
#         super().__init__()
#         self.include_pipeline_context = include_pipeline_context
#         self.include_user_context = include_user_context
#         self.include_session_metadata = include_session_metadata
#     
#     def filter(self, record):
#         # Add audit context from Kedro session and environment
#         if self.include_pipeline_context:
#             record.pipeline_name = getattr(record, 'pipeline_name', 'unknown')
#             record.node_name = getattr(record, 'node_name', 'unknown')
#         
#         if self.include_user_context:
#             record.user = getattr(record, 'user', 'system')
#         
#         if self.include_session_metadata:
#             record.session_id = getattr(record, 'session_id', 'unknown')
#         
#         return True

# ==============================================================================
# LOGGING BEST PRACTICES AND USAGE EXAMPLES
# ==============================================================================

# Example usage in FigRegistry-Kedro plugin components:
#
# import logging
# logger = logging.getLogger(__name__)
#
# # Performance logging with timing context
# start_time = time.time()
# # ... operation ...
# duration_ms = (time.time() - start_time) * 1000
# logger.info("Configuration bridge merge completed", extra={'duration_ms': duration_ms})
#
# # Audit logging with pipeline context
# logger.info("Figure saved with automated styling", extra={
#     'pipeline_name': pipeline_name,
#     'node_name': node_name,
#     'session_id': session_id,
#     'figure_path': figure_path,
#     'style_condition': condition
# })
#
# # Error logging with comprehensive context
# logger.error("Hook initialization failed", extra={
#     'error_type': type(e).__name__,
#     'error_message': str(e),
#     'pipeline_name': pipeline_name,
#     'initialization_phase': 'config_bridge_setup'
# }, exc_info=True)

# ==============================================================================
# INTEGRATION WITH KEDRO CONFIGURATION SYSTEM
# ==============================================================================

# This logging configuration integrates seamlessly with Kedro's configuration
# system and supports environment-specific overrides through the standard
# conf/{environment}/logging.yml pattern. The configuration provides:
#
# 1. Comprehensive visibility into FigRegistry plugin operations
# 2. Performance monitoring with SLA tracking capabilities  
# 3. Enterprise-grade audit trails for compliance requirements
# 4. Multi-environment support for development, staging, and production
# 5. Structured logging for integration with enterprise log aggregation systems
# 6. Advanced error correlation and debugging capabilities
#
# For custom deployment requirements, modify the handlers, formatters, and
# logger configurations according to your enterprise logging infrastructure
# and monitoring requirements.