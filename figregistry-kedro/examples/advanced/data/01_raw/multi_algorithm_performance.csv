experiment_id,algorithm_name,algorithm_type,model_class,hyperparameters,optimization_method,dataset_split,cross_validation_fold,environment,performance_tier,accuracy,precision,recall,f1_score,roc_auc,training_time_seconds,inference_latency_ms,memory_usage_mb,confidence_level,confidence_interval_lower,confidence_interval_upper,p_value,significance_level,effect_size,sample_size,random_seed,feature_count,model_parameters,validation_score,training_score,test_score,statistical_test,multiple_testing_correction,purpose,report_type,condition_id,researcher,institution,experiment_date,notes
advanced_ml_comparison_001,LogisticRegression,logistic_regression,baseline,"{""C"": 1.0, ""penalty"": ""l2"", ""solver"": ""lbfgs"", ""max_iter"": 1000}",grid_search,train,1,development,fair,0.7234,0.7156,0.7089,0.7122,0.7834,2.34,45.2,128.5,medium,0.6934,0.7534,0.1245,not_significant,small,1000,42,15,1501,0.7189,0.7245,0.7234,chi_square,bonferroni,exploration,technical,training.model.baseline.development,Dr. Sarah Chen,Advanced Analytics Institute,2024-12-19,"Baseline logistic regression model for development environment with standard regularization"
advanced_ml_comparison_002,LogisticRegression,logistic_regression,baseline,"{""C"": 0.1, ""penalty"": ""l1"", ""solver"": ""liblinear"", ""max_iter"": 1000}",grid_search,train,2,development,fair,0.7189,0.7098,0.7045,0.7071,0.7789,1.89,42.8,115.2,medium,0.6889,0.7489,0.1567,not_significant,small,1000,42,15,1501,0.7145,0.7201,0.7189,chi_square,bonferroni,exploration,technical,training.model.baseline.development,Dr. Sarah Chen,Advanced Analytics Institute,2024-12-19,"L1 regularized logistic regression with reduced overfitting"
advanced_ml_comparison_003,LogisticRegression,logistic_regression,baseline,"{""C"": 10.0, ""penalty"": ""l2"", ""solver"": ""lbfgs"", ""max_iter"": 1500}",grid_search,train,3,development,good,0.7456,0.7398,0.7334,0.7366,0.8023,3.12,47.6,134.8,medium,0.7156,0.7756,0.0987,not_significant,small,1000,42,15,1501,0.7412,0.7467,0.7456,chi_square,bonferroni,exploration,technical,training.model.baseline.development,Dr. Sarah Chen,Advanced Analytics Institute,2024-12-19,"Higher regularization strength showing improved generalization"
advanced_ml_comparison_004,RandomForest,random_forest,candidate,"{""n_estimators"": 100, ""max_depth"": 10, ""min_samples_split"": 5, ""random_state"": 42}",bayesian_optimization,train,1,development,good,0.8123,0.8089,0.8045,0.8067,0.8756,15.67,89.4,512.3,high,0.7823,0.8423,0.0234,p05,medium,1000,43,15,10100,0.8078,0.8134,0.8123,wilcoxon,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Michael Rodriguez,Data Science Research Center,2024-12-19,"Random forest with balanced tree depth and ensemble size"
advanced_ml_comparison_005,RandomForest,random_forest,candidate,"{""n_estimators"": 200, ""max_depth"": 15, ""min_samples_split"": 3, ""random_state"": 43}",bayesian_optimization,train,2,development,excellent,0.8567,0.8534,0.8489,0.8511,0.9123,28.45,156.7,1024.6,high,0.8267,0.8867,0.0089,p01,large,1000,43,15,20200,0.8523,0.8578,0.8567,wilcoxon,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Michael Rodriguez,Data Science Research Center,2024-12-19,"Deeper random forest with larger ensemble showing superior performance"
advanced_ml_comparison_006,RandomForest,random_forest,candidate,"{""n_estimators"": 300, ""max_depth"": 20, ""min_samples_split"": 2, ""random_state"": 44}",bayesian_optimization,train,3,development,excellent,0.8634,0.8598,0.8556,0.8577,0.9187,41.23,234.5,1536.9,high,0.8334,0.8934,0.0067,p01,large,1000,43,15,30300,0.8589,0.8645,0.8634,wilcoxon,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Michael Rodriguez,Data Science Research Center,2024-12-19,"Maximum depth random forest approaching overfitting boundary"
advanced_ml_comparison_007,GradientBoosting,gradient_boosting,candidate,"{""n_estimators"": 100, ""learning_rate"": 0.1, ""max_depth"": 6, ""random_state"": 45}",bayesian_optimization,train,1,development,excellent,0.8745,0.8712,0.8667,0.8689,0.9345,45.78,67.8,256.7,high,0.8445,0.9045,0.0034,p01,large,1000,44,15,600,0.8701,0.8756,0.8745,paired_t_test,holm_bonferroni,exploration,technical,training.model.candidate.development,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"Gradient boosting with optimal learning rate and tree depth"
advanced_ml_comparison_008,GradientBoosting,gradient_boosting,candidate,"{""n_estimators"": 150, ""learning_rate"": 0.05, ""max_depth"": 8, ""random_state"": 46}",bayesian_optimization,train,2,development,excellent,0.8812,0.8789,0.8734,0.8761,0.9412,67.89,89.3,384.5,high,0.8512,0.9112,0.0023,p01,large,1000,44,15,1200,0.8767,0.8823,0.8812,paired_t_test,holm_bonferroni,exploration,technical,training.model.candidate.development,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"Lower learning rate with deeper trees for improved stability"
advanced_ml_comparison_009,GradientBoosting,gradient_boosting,candidate,"{""n_estimators"": 200, ""learning_rate"": 0.01, ""max_depth"": 10, ""random_state"": 47}",bayesian_optimization,train,3,development,excellent,0.8756,0.8723,0.8689,0.8706,0.9378,89.45,112.6,512.8,high,0.8456,0.9056,0.0041,p01,large,1000,44,15,2000,0.8712,0.8767,0.8756,paired_t_test,holm_bonferroni,exploration,technical,training.model.candidate.development,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"Conservative learning rate requiring more estimators"
advanced_ml_comparison_010,MLPClassifier,neural_network,candidate,"{""hidden_layer_sizes"": [100, 50], ""activation"": ""relu"", ""solver"": ""adam"", ""max_iter"": 1000}",neural_architecture_search,train,1,development,good,0.8234,0.8189,0.8134,0.8161,0.8923,156.78,45.6,512.4,high,0.7934,0.8534,0.0156,p05,medium,1000,46,15,5150,0.8189,0.8245,0.8234,mcnemar,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Jennifer Liu,Deep Learning Research Institute,2024-12-19,"Two-layer neural network with ReLU activation"
advanced_ml_comparison_011,MLPClassifier,neural_network,candidate,"{""hidden_layer_sizes"": [200, 100, 50], ""activation"": ""tanh"", ""solver"": ""adam"", ""max_iter"": 1500}",neural_architecture_search,train,2,development,excellent,0.8567,0.8534,0.8489,0.8511,0.9234,245.67,67.8,1024.7,high,0.8267,0.8867,0.0078,p01,large,1000,46,15,17650,0.8523,0.8578,0.8567,mcnemar,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Jennifer Liu,Deep Learning Research Institute,2024-12-19,"Deeper neural network with tanh activation for complex patterns"
advanced_ml_comparison_012,MLPClassifier,neural_network,candidate,"{""hidden_layer_sizes"": [300, 200, 100], ""activation"": ""relu"", ""solver"": ""lbfgs"", ""max_iter"": 2000}",neural_architecture_search,train,3,development,excellent,0.8645,0.8612,0.8567,0.8589,0.9312,378.92,89.7,1536.9,high,0.8345,0.8945,0.0056,p01,large,1000,46,15,62100,0.8601,0.8656,0.8645,mcnemar,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Jennifer Liu,Deep Learning Research Institute,2024-12-19,"Large neural network with L-BFGS solver for batch optimization"
advanced_ml_comparison_013,VotingClassifier,ensemble,champion,"{""estimators"": [""rf"", ""gb"", ""mlp""], ""voting"": ""soft"", ""weights"": [1, 2, 1]}",ensemble_optimization,train,1,development,excellent,0.8934,0.8898,0.8856,0.8877,0.9567,89.45,156.7,1024.8,high,0.8634,0.9234,0.0012,p001,large,1000,45,15,32751,0.8889,0.8945,0.8934,cochran_q,bonferroni,exploration,technical,training.model.champion.development,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Soft voting ensemble combining random forest, gradient boosting, and neural network"
advanced_ml_comparison_014,VotingClassifier,ensemble,champion,"{""estimators"": [""rf"", ""gb"", ""mlp""], ""voting"": ""hard"", ""weights"": [1, 1, 1]}",ensemble_optimization,train,2,development,excellent,0.8823,0.8789,0.8734,0.8761,0.9478,78.34,134.2,896.5,high,0.8523,0.9123,0.0018,p01,large,1000,45,15,32751,0.8778,0.8834,0.8823,cochran_q,bonferroni,exploration,technical,training.model.champion.development,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Hard voting ensemble with equal weighting for robust predictions"
advanced_ml_comparison_015,VotingClassifier,ensemble,champion,"{""estimators"": [""rf"", ""gb"", ""mlp"", ""lr""], ""voting"": ""soft"", ""weights"": [2, 3, 1, 1]}",ensemble_optimization,train,3,development,excellent,0.8978,0.8945,0.8901,0.8923,0.9623,112.67,178.9,1280.4,high,0.8678,0.9278,0.0008,p001,large,1000,45,15,34252,0.8934,0.8989,0.8978,cochran_q,bonferroni,exploration,technical,training.model.champion.development,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Four-model ensemble with gradient boosting emphasis for maximum performance"
advanced_ml_comparison_016,LogisticRegression,logistic_regression,baseline,"{""C"": 1.0, ""penalty"": ""l2"", ""solver"": ""lbfgs"", ""max_iter"": 1000}",grid_search,validation,1,staging,good,0.7456,0.7423,0.7389,0.7406,0.8123,2.67,48.9,142.3,high,0.7156,0.7756,0.0867,not_significant,small,500,42,15,1501,0.7412,0.7467,0.7456,chi_square,bonferroni,presentation,technical,training.model.baseline.staging,Dr. Sarah Chen,Advanced Analytics Institute,2024-12-19,"Staging validation of baseline model showing improved performance"
advanced_ml_comparison_017,RandomForest,random_forest,candidate,"{""n_estimators"": 200, ""max_depth"": 15, ""min_samples_split"": 3, ""random_state"": 43}",bayesian_optimization,validation,1,staging,excellent,0.8678,0.8645,0.8601,0.8623,0.9234,31.23,167.8,1124.7,high,0.8378,0.8978,0.0067,p01,large,500,43,15,20200,0.8634,0.8689,0.8678,wilcoxon,fdr_bh,presentation,technical,training.model.candidate.staging,Dr. Michael Rodriguez,Data Science Research Center,2024-12-19,"Staging validation confirming random forest generalization"
advanced_ml_comparison_018,GradientBoosting,gradient_boosting,candidate,"{""n_estimators"": 150, ""learning_rate"": 0.05, ""max_depth"": 8, ""random_state"": 46}",bayesian_optimization,validation,1,staging,excellent,0.8834,0.8801,0.8767,0.8784,0.9345,72.45,95.6,398.7,high,0.8534,0.9134,0.0023,p01,large,500,44,15,1200,0.8789,0.8845,0.8834,paired_t_test,holm_bonferroni,presentation,technical,training.model.candidate.staging,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"Staging validation showing gradient boosting consistency"
advanced_ml_comparison_019,MLPClassifier,neural_network,candidate,"{""hidden_layer_sizes"": [200, 100, 50], ""activation"": ""tanh"", ""solver"": ""adam"", ""max_iter"": 1500}",neural_architecture_search,validation,1,staging,excellent,0.8589,0.8556,0.8512,0.8534,0.9167,267.89,72.4,1087.3,high,0.8289,0.8889,0.0089,p01,large,500,46,15,17650,0.8545,0.8600,0.8589,mcnemar,fdr_bh,presentation,technical,training.model.candidate.staging,Dr. Jennifer Liu,Deep Learning Research Institute,2024-12-19,"Neural network staging validation with stable performance"
advanced_ml_comparison_020,VotingClassifier,ensemble,champion,"{""estimators"": [""rf"", ""gb"", ""mlp"", ""lr""], ""voting"": ""soft"", ""weights"": [2, 3, 1, 1]}",ensemble_optimization,validation,1,staging,excellent,0.9012,0.8978,0.8934,0.8956,0.9656,123.45,189.7,1356.8,high,0.8712,0.9312,0.0006,p001,large,500,45,15,34252,0.8967,0.9023,0.9012,cochran_q,bonferroni,presentation,technical,training.model.champion.staging,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Staging validation confirming ensemble superiority"
advanced_ml_comparison_021,LogisticRegression,logistic_regression,baseline,"{""C"": 1.0, ""penalty"": ""l2"", ""solver"": ""lbfgs"", ""max_iter"": 1000}",grid_search,test,1,production,good,0.7389,0.7356,0.7323,0.7339,0.8067,3.12,52.3,156.7,high,0.7089,0.7689,0.0945,not_significant,small,250,42,15,1501,0.7345,0.7400,0.7389,chi_square,bonferroni,publication,executive,training.model.baseline.production,Dr. Sarah Chen,Advanced Analytics Institute,2024-12-19,"Production baseline with consistent cross-environment performance"
advanced_ml_comparison_022,RandomForest,random_forest,candidate,"{""n_estimators"": 200, ""max_depth"": 15, ""min_samples_split"": 3, ""random_state"": 43}",bayesian_optimization,test,1,production,excellent,0.8612,0.8578,0.8534,0.8556,0.9189,34.56,178.9,1189.4,high,0.8312,0.8912,0.0078,p01,large,250,43,15,20200,0.8567,0.8623,0.8612,wilcoxon,fdr_bh,publication,executive,training.model.candidate.production,Dr. Michael Rodriguez,Data Science Research Center,2024-12-19,"Production random forest meeting enterprise performance standards"
advanced_ml_comparison_023,GradientBoosting,gradient_boosting,candidate,"{""n_estimators"": 150, ""learning_rate"": 0.05, ""max_depth"": 8, ""random_state"": 46}",bayesian_optimization,test,1,production,excellent,0.8767,0.8734,0.8689,0.8711,0.9312,78.90,102.4,423.8,high,0.8467,0.9067,0.0034,p01,large,250,44,15,1200,0.8723,0.8778,0.8767,paired_t_test,holm_bonferroni,publication,executive,training.model.candidate.production,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"Production gradient boosting with sub-100ms latency requirement"
advanced_ml_comparison_024,MLPClassifier,neural_network,candidate,"{""hidden_layer_sizes"": [200, 100, 50], ""activation"": ""tanh"", ""solver"": ""adam"", ""max_iter"": 1500}",neural_architecture_search,test,1,production,excellent,0.8523,0.8489,0.8445,0.8467,0.9123,289.67,78.9,1156.2,high,0.8223,0.8823,0.0123,p05,medium,250,46,15,17650,0.8478,0.8534,0.8523,mcnemar,fdr_bh,publication,executive,training.model.candidate.production,Dr. Jennifer Liu,Deep Learning Research Institute,2024-12-19,"Production neural network optimized for inference latency"
advanced_ml_comparison_025,VotingClassifier,ensemble,champion,"{""estimators"": [""rf"", ""gb"", ""mlp"", ""lr""], ""voting"": ""soft"", ""weights"": [2, 3, 1, 1]}",ensemble_optimization,test,1,production,excellent,0.8945,0.8912,0.8867,0.8889,0.9589,134.78,201.5,1423.6,high,0.8645,0.9245,0.0004,p001,large,250,45,15,34252,0.8901,0.8956,0.8945,cochran_q,bonferroni,publication,executive,training.model.champion.production,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Production champion ensemble delivering enterprise-grade performance"
advanced_ml_comparison_026,LogisticRegression,logistic_regression,baseline,"{""C"": 0.5, ""penalty"": ""elasticnet"", ""solver"": ""saga"", ""l1_ratio"": 0.5}",grid_search,train,1,development,fair,0.7167,0.7123,0.7089,0.7106,0.7789,3.45,49.8,134.6,medium,0.6867,0.7467,0.1456,not_significant,small,1000,42,15,1501,0.7123,0.7178,0.7167,chi_square,bonferroni,exploration,technical,training.model.baseline.development,Dr. Sarah Chen,Advanced Analytics Institute,2024-12-19,"Elastic net regularization combining L1 and L2 penalties"
advanced_ml_comparison_027,SVC,support_vector_machine,candidate,"{""C"": 1.0, ""kernel"": ""rbf"", ""gamma"": ""scale"", ""probability"": true}",grid_search,train,1,development,good,0.8345,0.8312,0.8267,0.8289,0.8967,67.89,23.4,256.8,high,0.8045,0.8645,0.0234,p05,medium,1000,48,15,1000,0.8301,0.8356,0.8345,mann_whitney,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Robert Kim,Comparative Algorithm Research Lab,2024-12-19,"Support vector machine with RBF kernel for non-linear patterns"
advanced_ml_comparison_028,SVC,support_vector_machine,candidate,"{""C"": 10.0, ""kernel"": ""poly"", ""degree"": 3, ""probability"": true}",grid_search,train,2,development,excellent,0.8567,0.8534,0.8489,0.8511,0.9123,89.45,34.7,384.5,high,0.8267,0.8867,0.0089,p01,large,1000,48,15,3000,0.8523,0.8578,0.8567,mann_whitney,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Robert Kim,Comparative Algorithm Research Lab,2024-12-19,"Polynomial kernel SVM capturing complex feature interactions"
advanced_ml_comparison_029,XGBClassifier,gradient_boosting,candidate,"{""n_estimators"": 100, ""learning_rate"": 0.1, ""max_depth"": 6, ""subsample"": 0.8}",bayesian_optimization,train,1,development,excellent,0.8789,0.8756,0.8712,0.8734,0.9389,34.67,78.9,298.7,high,0.8489,0.9089,0.0023,p01,large,1000,49,15,600,0.8745,0.8800,0.8789,paired_t_test,holm_bonferroni,exploration,technical,training.model.candidate.development,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"XGBoost with optimized hyperparameters for efficiency"
advanced_ml_comparison_030,XGBClassifier,gradient_boosting,candidate,"{""n_estimators"": 200, ""learning_rate"": 0.05, ""max_depth"": 8, ""subsample"": 0.9}",bayesian_optimization,train,2,development,excellent,0.8856,0.8823,0.8789,0.8806,0.9434,56.78,89.3,456.8,high,0.8556,0.9156,0.0012,p001,large,1000,49,15,1600,0.8812,0.8867,0.8856,paired_t_test,holm_bonferroni,exploration,technical,training.model.candidate.development,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"XGBoost with deeper trees and conservative learning rate"
advanced_ml_comparison_031,LGBMClassifier,gradient_boosting,candidate,"{""n_estimators"": 100, ""learning_rate"": 0.1, ""max_depth"": 6, ""num_leaves"": 31}",bayesian_optimization,train,1,development,excellent,0.8723,0.8689,0.8645,0.8667,0.9356,12.45,56.7,189.4,high,0.8423,0.9023,0.0034,p01,large,1000,50,15,3100,0.8678,0.8734,0.8723,paired_t_test,holm_bonferroni,exploration,technical,training.model.candidate.development,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"LightGBM optimized for speed and memory efficiency"
advanced_ml_comparison_032,LGBMClassifier,gradient_boosting,candidate,"{""n_estimators"": 150, ""learning_rate"": 0.08, ""max_depth"": 8, ""num_leaves"": 63}",bayesian_optimization,train,2,development,excellent,0.8801,0.8767,0.8734,0.8750,0.9401,18.67,67.8,245.6,high,0.8501,0.9101,0.0018,p01,large,1000,50,15,9450,0.8756,0.8812,0.8801,paired_t_test,holm_bonferroni,exploration,technical,training.model.candidate.development,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"LightGBM with increased complexity for improved accuracy"
advanced_ml_comparison_033,CatBoostClassifier,gradient_boosting,candidate,"{""iterations"": 100, ""learning_rate"": 0.1, ""depth"": 6, ""l2_leaf_reg"": 3}",bayesian_optimization,train,1,development,excellent,0.8678,0.8645,0.8601,0.8623,0.9323,23.45,45.6,156.8,high,0.8378,0.8978,0.0045,p01,large,1000,51,15,600,0.8634,0.8689,0.8678,paired_t_test,holm_bonferroni,exploration,technical,training.model.candidate.development,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"CatBoost with automatic categorical feature handling"
advanced_ml_comparison_034,CatBoostClassifier,gradient_boosting,candidate,"{""iterations"": 200, ""learning_rate"": 0.05, ""depth"": 8, ""l2_leaf_reg"": 1}",bayesian_optimization,train,2,development,excellent,0.8734,0.8701,0.8667,0.8684,0.9367,45.89,56.7,234.5,high,0.8434,0.9034,0.0028,p01,large,1000,51,15,1600,0.8689,0.8745,0.8734,paired_t_test,holm_bonferroni,exploration,technical,training.model.candidate.development,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"CatBoost with reduced regularization for complex patterns"
advanced_ml_comparison_035,AdaBoostClassifier,ensemble,candidate,"{""n_estimators"": 100, ""learning_rate"": 1.0, ""algorithm"": ""SAMME.R"", ""random_state"": 52}",bayesian_optimization,train,1,development,good,0.8234,0.8201,0.8167,0.8184,0.8934,45.67,89.4,298.7,high,0.7934,0.8534,0.0156,p05,medium,1000,52,15,10000,0.8189,0.8245,0.8234,wilcoxon,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"AdaBoost with SAMME.R algorithm for probability estimates"
advanced_ml_comparison_036,AdaBoostClassifier,ensemble,candidate,"{""n_estimators"": 200, ""learning_rate"": 0.5, ""algorithm"": ""SAMME"", ""random_state"": 53}",bayesian_optimization,train,2,development,excellent,0.8456,0.8423,0.8389,0.8406,0.9067,78.90,134.6,456.8,high,0.8156,0.8756,0.0098,p01,medium,1000,52,15,20000,0.8412,0.8467,0.8456,wilcoxon,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"AdaBoost with reduced learning rate for stable convergence"
advanced_ml_comparison_037,ExtraTreesClassifier,random_forest,candidate,"{""n_estimators"": 100, ""max_depth"": 15, ""min_samples_split"": 2, ""random_state"": 54}",bayesian_optimization,train,1,development,excellent,0.8567,0.8534,0.8501,0.8517,0.9189,8.90,167.8,512.4,high,0.8267,0.8867,0.0067,p01,large,1000,54,15,15000,0.8523,0.8578,0.8567,wilcoxon,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Michael Rodriguez,Data Science Research Center,2024-12-19,"Extra trees with random splits for reduced variance"
advanced_ml_comparison_038,ExtraTreesClassifier,random_forest,candidate,"{""n_estimators"": 200, ""max_depth"": 20, ""min_samples_split"": 3, ""random_state"": 55}",bayesian_optimization,train,2,development,excellent,0.8634,0.8601,0.8567,0.8584,0.9234,16.78,234.5,1024.7,high,0.8334,0.8934,0.0045,p01,large,1000,54,15,40000,0.8589,0.8645,0.8634,wilcoxon,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Michael Rodriguez,Data Science Research Center,2024-12-19,"Extra trees with deeper architecture for complex patterns"
advanced_ml_comparison_039,BaggingClassifier,ensemble,candidate,"{""n_estimators"": 100, ""max_samples"": 0.8, ""max_features"": 0.8, ""random_state"": 56}",bayesian_optimization,train,1,development,good,0.8123,0.8089,0.8056,0.8072,0.8823,34.56,123.4,398.7,high,0.7823,0.8423,0.0189,p05,medium,1000,56,15,10000,0.8078,0.8134,0.8123,wilcoxon,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Bagging classifier with subsampling for variance reduction"
advanced_ml_comparison_040,BaggingClassifier,ensemble,candidate,"{""n_estimators"": 200, ""max_samples"": 0.9, ""max_features"": 0.9, ""random_state"": 57}",bayesian_optimization,train,2,development,excellent,0.8345,0.8312,0.8278,0.8295,0.8989,67.89,189.7,678.9,high,0.8045,0.8645,0.0123,p05,medium,1000,56,15,20000,0.8301,0.8356,0.8345,wilcoxon,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Bagging with increased sampling for better base model diversity"
advanced_ml_comparison_041,StackingClassifier,ensemble,champion,"{""estimators"": [""rf"", ""gb"", ""mlp""], ""final_estimator"": ""LogisticRegression"", ""cv"": 5}",ensemble_optimization,train,1,development,excellent,0.9012,0.8978,0.8945,0.8961,0.9634,156.78,234.5,1567.8,high,0.8712,0.9312,0.0008,p001,large,1000,57,15,34252,0.8967,0.9023,0.9012,cochran_q,bonferroni,exploration,technical,training.model.champion.development,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Stacking ensemble with logistic regression meta-learner"
advanced_ml_comparison_042,StackingClassifier,ensemble,champion,"{""estimators"": [""rf"", ""gb"", ""xgb"", ""lgb""], ""final_estimator"": ""RandomForestClassifier"", ""cv"": 3}",ensemble_optimization,train,2,development,excellent,0.9078,0.9045,0.9012,0.9028,0.9689,234.56,298.7,2048.9,high,0.8778,0.9378,0.0005,p001,large,1000,57,15,38852,0.9034,0.9089,0.9078,cochran_q,bonferroni,exploration,technical,training.model.champion.development,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Advanced stacking with gradient boosting variants and RF meta-learner"
advanced_ml_comparison_043,LogisticRegression,logistic_regression,baseline,"{""C"": 1.0, ""penalty"": ""l2"", ""solver"": ""lbfgs"", ""max_iter"": 1000}",grid_search,train,1,staging,good,0.7423,0.7389,0.7356,0.7372,0.8089,2.78,51.2,145.6,high,0.7123,0.7723,0.0923,not_significant,small,2500,42,15,1501,0.7378,0.7434,0.7423,chi_square,bonferroni,presentation,technical,training.model.baseline.staging,Dr. Sarah Chen,Advanced Analytics Institute,2024-12-19,"Staging baseline with larger dataset validation"
advanced_ml_comparison_044,VotingClassifier,ensemble,champion,"{""estimators"": [""rf"", ""gb"", ""xgb"", ""lgb""], ""voting"": ""soft"", ""weights"": [1, 2, 2, 1]}",ensemble_optimization,train,1,staging,excellent,0.9145,0.9112,0.9078,0.9095,0.9734,167.89,256.7,1789.4,high,0.8845,0.9445,0.0003,p001,large,2500,45,15,38852,0.9101,0.9156,0.9145,cochran_q,bonferroni,presentation,technical,training.model.champion.staging,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Staging champion ensemble with four gradient boosting variants"
advanced_ml_comparison_045,VotingClassifier,ensemble,champion,"{""estimators"": [""rf"", ""gb"", ""xgb"", ""lgb""], ""voting"": ""soft"", ""weights"": [1, 2, 2, 1]}",ensemble_optimization,test,1,production,excellent,0.9089,0.9056,0.9023,0.9039,0.9701,178.45,267.8,1823.7,high,0.8789,0.9389,0.0004,p001,large,1250,45,15,38852,0.9045,0.9100,0.9089,cochran_q,bonferroni,publication,executive,training.model.champion.production,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Production champion ensemble delivering maximum performance with enterprise latency"
advanced_ml_comparison_046,LogisticRegression,logistic_regression,baseline,"{""C"": 1.0, ""penalty"": ""l2"", ""solver"": ""lbfgs"", ""max_iter"": 1000}",grid_search,train,4,development,fair,0.7201,0.7167,0.7134,0.7150,0.7823,2.45,46.8,131.2,medium,0.6901,0.7501,0.1345,not_significant,small,1000,42,15,1501,0.7156,0.7212,0.7201,chi_square,bonferroni,exploration,technical,training.model.baseline.development,Dr. Sarah Chen,Advanced Analytics Institute,2024-12-19,"Cross-validation fold 4 showing baseline consistency"
advanced_ml_comparison_047,RandomForest,random_forest,candidate,"{""n_estimators"": 200, ""max_depth"": 15, ""min_samples_split"": 3, ""random_state"": 43}",bayesian_optimization,train,4,development,excellent,0.8589,0.8556,0.8523,0.8539,0.9156,29.67,162.3,1089.6,high,0.8289,0.8889,0.0078,p01,large,1000,43,15,20200,0.8545,0.8600,0.8589,wilcoxon,fdr_bh,exploration,technical,training.model.candidate.development,Dr. Michael Rodriguez,Data Science Research Center,2024-12-19,"Cross-validation fold 4 confirming random forest stability"
advanced_ml_comparison_048,GradientBoosting,gradient_boosting,candidate,"{""n_estimators"": 150, ""learning_rate"": 0.05, ""max_depth"": 8, ""random_state"": 46}",bayesian_optimization,train,4,development,excellent,0.8823,0.8789,0.8756,0.8772,0.9423,69.78,91.2,412.6,high,0.8523,0.9123,0.0028,p01,large,1000,44,15,1200,0.8778,0.8834,0.8823,paired_t_test,holm_bonferroni,exploration,technical,training.model.candidate.development,Dr. Emily Zhang,Machine Learning Innovation Lab,2024-12-19,"Cross-validation fold 4 gradient boosting performance validation"
advanced_ml_comparison_049,VotingClassifier,ensemble,champion,"{""estimators"": [""rf"", ""gb"", ""xgb"", ""lgb""], ""voting"": ""soft"", ""weights"": [1, 2, 2, 1]}",ensemble_optimization,train,4,development,excellent,0.8989,0.8956,0.8923,0.8939,0.9645,145.67,198.4,1456.8,high,0.8689,0.9289,0.0006,p001,large,1000,45,15,38852,0.8945,0.9000,0.8989,cochran_q,bonferroni,exploration,technical,training.model.champion.development,Dr. Alex Thompson,Ensemble Learning Research Group,2024-12-19,"Cross-validation fold 4 champion ensemble maintaining excellence"
advanced_ml_comparison_050,LogisticRegression,logistic_regression,baseline,"{""C"": 1.0, ""penalty"": ""l2"", ""solver"": ""lbfgs"", ""max_iter"": 1000}",grid_search,train,5,development,fair,0.7178,0.7145,0.7112,0.7128,0.7801,2.56,47.3,133.8,medium,0.6878,0.7478,0.1423,not_significant,small,1000,42,15,1501,0.7134,0.7189,0.7178,chi_square,bonferroni,exploration,technical,training.model.baseline.development,Dr. Sarah Chen,Advanced Analytics Institute,2024-12-19,"Cross-validation fold 5 baseline completing validation cycle"